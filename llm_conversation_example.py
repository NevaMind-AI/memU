"""
ğŸš€ LLMé©±åŠ¨çš„PersonaLab Conversationç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å®Œå…¨åŸºäºLLMçš„Memoryæ›´æ–°ç³»ç»Ÿï¼Œä¸ä½¿ç”¨ä»»ä½•è§„åˆ™æ€§é€»è¾‘
"""

from personalab.memory import (
    MemoryManager, 
    create_llm_client,
    OpenAIClient,
    MockLLMClient
)

def create_llm_memory_manager(client_type="mock", **llm_config):
    """
    åˆ›å»ºä½¿ç”¨LLMçš„MemoryManager
    
    Args:
        client_type: LLMå®¢æˆ·ç«¯ç±»å‹ ("mock", "openai")
        **llm_config: LLMé…ç½®å‚æ•°
    """
    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = create_llm_client(client_type, **llm_config)
    
    # åˆ›å»ºä½¿ç”¨LLMçš„MemoryManager
    memory_manager = MemoryManager(
        db_path=f"llm_{client_type}_memory.db",
        llm_client=llm_client,
        temperature=0.3,        # LLMå‚æ•°
        max_tokens=2000
    )
    
    return memory_manager


def llm_conversation_example():
    """LLMé©±åŠ¨çš„conversationå¤„ç†ç¤ºä¾‹"""
    print("ğŸ¤– LLMé©±åŠ¨çš„Memoryæ›´æ–°ç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºLLMé©±åŠ¨çš„Memoryç®¡ç†å™¨
    memory_manager = create_llm_memory_manager("mock")
    
    agent_id = "llm_agent_001"
    
    # ç¤ºä¾‹conversation
    conversation = [
        {
            'role': 'user', 
            'content': 'ä½ å¥½ï¼æˆ‘æ˜¯ææ˜ï¼Œä»Šå¹´28å²ï¼Œåœ¨ä¸Šæµ·åšå‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆã€‚æˆ‘ç‰¹åˆ«å–œæ¬¢Reactå’ŒVue.jsæ¡†æ¶ã€‚'
        },
        {
            'role': 'assistant', 
            'content': 'ä½ å¥½ææ˜ï¼å‰ç«¯å¼€å‘æ˜¯å¾ˆæœ‰è¶£çš„å·¥ä½œï¼ŒReactå’ŒVueéƒ½æ˜¯å¾ˆæµè¡Œçš„æ¡†æ¶ã€‚ä½ åœ¨ä¸Šæµ·çš„å“ªä¸ªåŒºåŸŸå·¥ä½œå‘¢ï¼Ÿ'
        },
        {
            'role': 'user', 
            'content': 'æˆ‘åœ¨æµ¦ä¸œæ–°åŒºçš„ä¸€å®¶äº’è”ç½‘å…¬å¸å·¥ä½œï¼Œä¸»è¦åšç”µå•†å¹³å°çš„å‰ç«¯å¼€å‘ã€‚æœ€è¿‘åœ¨å­¦ä¹ TypeScriptå’ŒNext.jsã€‚'
        },
        {
            'role': 'assistant', 
            'content': 'TypeScriptç¡®å®æ˜¯å‰ç«¯å¼€å‘çš„è¶‹åŠ¿ï¼ŒNext.jsä¹Ÿæ˜¯Reactç”Ÿæ€ä¸­å¾ˆå¼ºå¤§çš„æ¡†æ¶ã€‚ç”µå•†å¹³å°å¯¹ç”¨æˆ·ä½“éªŒè¦æ±‚å¾ˆé«˜å§ï¼Ÿ'
        },
        {
            'role': 'user', 
            'content': 'æ˜¯çš„ï¼Œæˆ‘ä»¬ç‰¹åˆ«æ³¨é‡æ€§èƒ½ä¼˜åŒ–å’Œç”¨æˆ·ä½“éªŒã€‚é™¤äº†å·¥ä½œï¼Œæˆ‘è¿˜å–œæ¬¢å‚åŠ æŠ€æœ¯meetupï¼Œç»å¸¸åœ¨æ˜é‡‘å’ŒGitHubä¸Šåˆ†äº«ä»£ç ã€‚'
        },
        {
            'role': 'assistant', 
            'content': 'å¾ˆæ£’ï¼æŠ€æœ¯åˆ†äº«æ˜¯å¾ˆå¥½çš„å­¦ä¹ æ–¹å¼ã€‚ä½ åœ¨GitHubä¸Šæœ‰ä»€ä¹ˆæ¯”è¾ƒæœ‰è¶£çš„é¡¹ç›®å—ï¼Ÿ'
        }
    ]
    
    print("ğŸ“ è¾“å…¥conversation:")
    print("-" * 40)
    for i, msg in enumerate(conversation, 1):
        role_emoji = "ğŸ‘¤" if msg['role'] == 'user' else "ğŸ¤–"
        print(f"{i}. {role_emoji} {msg['role']}: {msg['content'][:50]}...")
    
    print(f"\nğŸ”„ ä½¿ç”¨LLM Pipelineå¤„ç†conversation...")
    
    # ä½¿ç”¨LLM pipelineå¤„ç†conversation
    updated_memory, llm_result = memory_manager.update_memory_with_conversation(
        agent_id, conversation
    )
    
    print(f"\nâœ… LLMå¤„ç†å®Œæˆï¼")
    print("-" * 40)
    
    # æ˜¾ç¤ºLLM pipelineç»“æœ
    print(f"ğŸ“Š LLM Pipelineç»“æœ:")
    print(f"- ä½¿ç”¨æ¨¡å‹: {llm_result.pipeline_metadata.get('llm_model', 'unknown')}")
    print(f"- ç”»åƒæ›´æ–°: {llm_result.update_result.profile_updated}")
    print(f"- äº‹ä»¶æ·»åŠ : {llm_result.update_result.events_added}")
    print(f"- åˆ†æç½®ä¿¡åº¦: {llm_result.modification_result.analysis_confidence:.2f}")
    print(f"- ToMç½®ä¿¡åº¦: {llm_result.tom_result.confidence_score:.2f}")
    
    # æ˜¾ç¤ºLLMæå–çš„ä¿¡æ¯
    print(f"\nğŸ§  LLMæå–çš„ç”»åƒæ›´æ–°:")
    print("-" * 30)
    for i, update in enumerate(llm_result.modification_result.profile_updates, 1):
        print(f"{i}. {update}")
    
    print(f"\nğŸ“ LLMæå–çš„äº‹ä»¶:")
    print("-" * 30)
    for i, event in enumerate(llm_result.modification_result.events, 1):
        print(f"{i}. {event}")
    
    # æ˜¾ç¤ºLLMæ›´æ–°åçš„ç”»åƒ
    print(f"\nğŸ‘¤ LLMæ›´æ–°åçš„ç”¨æˆ·ç”»åƒ:")
    print("-" * 40)
    print(updated_memory.get_profile_content())
    
    # æ˜¾ç¤ºLLMçš„Theory of Mindåˆ†æ
    print(f"\nğŸ§  LLM Theory of Mindåˆ†æ:")
    print("-" * 40)
    tom_insights = llm_result.tom_result.insights
    
    if 'intent_analysis' in tom_insights:
        intent = tom_insights['intent_analysis']
        print(f"ğŸ’­ æ„å›¾åˆ†æ: {intent.get('primary_intent', 'unknown')}")
    
    if 'emotion_analysis' in tom_insights:
        emotion = tom_insights['emotion_analysis']
        print(f"ğŸ˜Š æƒ…ç»ªåˆ†æ: {emotion.get('dominant_emotion', 'unknown')}")
    
    if 'behavior_patterns' in tom_insights:
        behavior = tom_insights['behavior_patterns']
        print(f"ğŸ¯ è¡Œä¸ºæ¨¡å¼: {behavior.get('communication_style', 'unknown')}")
        print(f"ğŸ“ˆ å‚ä¸åº¦: {behavior.get('engagement_level', 'unknown')}")
    
    if 'cognitive_state' in tom_insights:
        cognitive = tom_insights['cognitive_state']
        print(f"ğŸ“ çŸ¥è¯†æ°´å¹³: {cognitive.get('knowledge_level', 'unknown')}")
        print(f"ğŸ“š å­¦ä¹ é£æ ¼: {cognitive.get('learning_style', 'unknown')}")
    
    # æ˜¾ç¤ºå®Œæ•´çš„Memory prompt
    print(f"\nğŸ“‹ å®Œæ•´Memory Prompt (LLMç”Ÿæˆ):")
    print("=" * 60)
    memory_prompt = updated_memory.to_prompt()
    print(memory_prompt)
    print("=" * 60)
    
    # æ˜¾ç¤ºåŸå§‹LLMå“åº”
    print(f"\nğŸ” LLMåŸå§‹å“åº” (è°ƒè¯•ç”¨):")
    print("-" * 40)
    print("åˆ†æé˜¶æ®µå“åº”:")
    print(llm_result.modification_result.raw_llm_response[:200] + "...")
    print("\næ›´æ–°é˜¶æ®µå“åº”:")
    print(llm_result.update_result.updated_profile_content[:200] + "...")
    print("\nToMåˆ†æå“åº”:")
    print(llm_result.tom_result.raw_llm_response[:200] + "...")


def compare_pipelines_example():
    """å¯¹æ¯”LLM pipelineå’Œè§„åˆ™pipelineçš„ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ğŸ”„ LLM Pipeline vs è§„åˆ™Pipelineå¯¹æ¯”")
    print("=" * 60)
    
    # åŒä¸€ä¸ªconversation
    conversation = [
        {'role': 'user', 'content': 'æˆ‘æ˜¯ç‹å°æ˜ï¼Œå–œæ¬¢ç¼–ç¨‹å’ŒéŸ³ä¹'},
        {'role': 'assistant', 'content': 'ç¼–ç¨‹å’ŒéŸ³ä¹éƒ½æ˜¯å¾ˆæœ‰åˆ›æ„çš„çˆ±å¥½ï¼'},
        {'role': 'user', 'content': 'æ˜¯çš„ï¼Œæˆ‘ç”¨Pythonå†™ä»£ç ï¼Œä¸šä½™æ—¶é—´å¼¹å‰ä»–'},
        {'role': 'assistant', 'content': 'Pythonå¾ˆæ£’ï¼ä½ å¼¹å‰ä»–å¤šä¹…äº†ï¼Ÿ'}
    ]
    
    # 1. LLM Pipeline
    print("ğŸ¤– LLM Pipelineå¤„ç†ç»“æœ:")
    print("-" * 30)
    
    llm_manager = create_llm_memory_manager("mock")
    llm_memory, llm_result = llm_manager.update_memory_with_conversation(
        "compare_llm", conversation
    )
    
    print(f"ç”»åƒ: {llm_memory.get_profile_content()}")
    print(f"äº‹ä»¶æ•°: {len(llm_memory.get_event_content())}")
    print(f"ToMæ´å¯Ÿ: {list(llm_result.tom_result.insights.keys())}")
    
    # 2. è§„åˆ™Pipeline
    print(f"\nğŸ“ è§„åˆ™Pipelineå¤„ç†ç»“æœ:")
    print("-" * 30)
    
    # æ³¨æ„ï¼šç°åœ¨å·²ç»æ²¡æœ‰è§„åˆ™pipelineäº†ï¼Œåªæœ‰LLM pipeline
    # è¿™é‡Œåªæ˜¯ä¸ºäº†æ¼”ç¤ºå¯¹æ¯”ï¼Œå®é™…ä¸Šéƒ½æ˜¯LLMé©±åŠ¨
    rule_manager = MemoryManager(
        db_path="rule_memory.db"
    )
    rule_memory, rule_result = rule_manager.update_memory_with_conversation(
        "compare_rule", conversation
    )
    
    print(f"ç”»åƒ: {rule_memory.get_profile_content()}")
    print(f"äº‹ä»¶æ•°: {len(rule_memory.get_event_content())}")
    print(f"ToMæ´å¯Ÿ: {list(rule_result.tom_result.insights.keys())}")
    
    print(f"\nğŸ’¡ å¯¹æ¯”æ€»ç»“:")
    print(f"- ç°åœ¨PersonaLabç»Ÿä¸€ä½¿ç”¨LLM Pipeline")
    print(f"- æ‰€æœ‰Memoryæ›´æ–°éƒ½æ˜¯æ™ºèƒ½ã€è‡ªç„¶çš„")


def openai_example():
    """ä½¿ç”¨OpenAI APIçš„ç¤ºä¾‹ï¼ˆéœ€è¦API Keyï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ”‘ OpenAI APIç¤ºä¾‹ (éœ€è¦API Key)")
    print("=" * 60)
    
    # æ³¨æ„ï¼šè¿™éœ€è¦çœŸå®çš„OpenAI API Key
    api_key = "your-openai-api-key-here"  # æ›¿æ¢ä¸ºçœŸå®çš„API Key
    
    if api_key == "your-openai-api-key-here":
        print("âš ï¸  è¯·è®¾ç½®çœŸå®çš„OpenAI API Keyæ‰èƒ½è¿è¡Œæ­¤ç¤ºä¾‹")
        return
    
    try:
        # åˆ›å»ºOpenAIé©±åŠ¨çš„Memoryç®¡ç†å™¨
        openai_manager = MemoryManager(
            db_path="openai_memory.db",
            llm_client=create_llm_client("openai", api_key=api_key),
            temperature=0.3,
            max_tokens=1500
        )
        
        conversation = [
            {'role': 'user', 'content': 'æˆ‘æ˜¯ä¸€åæ•°æ®ç§‘å­¦å®¶ï¼Œä¸“æ³¨äºæœºå™¨å­¦ä¹ '},
            {'role': 'assistant', 'content': 'æ•°æ®ç§‘å­¦æ˜¯å¾ˆæœ‰å‰æ™¯çš„é¢†åŸŸï¼'},
        ]
        
        memory, result = openai_manager.update_memory_with_conversation(
            "openai_user", conversation
        )
        
        print("âœ… OpenAIå¤„ç†æˆåŠŸï¼")
        print(f"ç”»åƒ: {memory.get_profile_content()}")
        
    except Exception as e:
        print(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥: {e}")


def simple_llm_usage():
    """æœ€ç®€å•çš„LLMä½¿ç”¨æ–¹å¼"""
    print("\n" + "=" * 60)
    print("âš¡ æœ€ç®€å•çš„LLMä½¿ç”¨æ–¹å¼")
    print("=" * 60)
    
    # ä¸€è¡Œä»£ç åˆ›å»ºMemoryç®¡ç†å™¨ï¼ˆé»˜è®¤LLMé©±åŠ¨ï¼‰
    manager = MemoryManager()
    
    # ä½ çš„conversation
    conversation = [
        {'role': 'user', 'content': 'æˆ‘å«å¼ ä¸‰ï¼Œæ˜¯ä¸ªç¨‹åºå‘˜'},
        {'role': 'assistant', 'content': 'ä½ å¥½å¼ ä¸‰ï¼'},
    ]
    
    # ä¸€è¡Œä»£ç å¤„ç†
    memory, _ = manager.update_memory_with_conversation("simple", conversation)
    
    # è·å–ç»“æœ
    prompt = memory.to_prompt()
    print("ğŸ¯ ç»“æœ:")
    print(prompt)


if __name__ == "__main__":
    # è¿è¡Œä¸»è¦ç¤ºä¾‹
    llm_conversation_example()
    
    # è¿è¡Œå¯¹æ¯”ç¤ºä¾‹
    compare_pipelines_example()
    
    # è¿è¡Œç®€å•ä½¿ç”¨ç¤ºä¾‹
    simple_llm_usage()
    
    # OpenAIç¤ºä¾‹ï¼ˆéœ€è¦API Keyï¼‰
    # openai_example()
    
    print(f"\nğŸ‰ LLMé©±åŠ¨çš„Memoryæ›´æ–°ç¤ºä¾‹å®Œæˆï¼")
    print(f"ğŸ’¡ ç°åœ¨PersonaLabå®Œå…¨ä½¿ç”¨LLMæ¥è¿›è¡ŒMemoryåˆ†æå’Œæ›´æ–°ï¼Œä¸å†ä¾èµ–è§„åˆ™æ€§é€»è¾‘ï¼") 
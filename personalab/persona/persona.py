"""
PersonaLab Persona Class

æä¾›ç®€æ´çš„APIæ¥ä½¿ç”¨PersonaLabçš„Memoryå’ŒMemoåŠŸèƒ½ï¼Œå¹¶é›†æˆLLMå¯¹è¯ã€‚
"""

import json
from typing import List, Dict, Optional, Union, Any, Callable
from contextlib import contextmanager

from ..memory import Memory
from ..memo import Memo
from ..llm import OpenAIClient, AnthropicClient, CustomLLMClient
from ..config import config

class Persona:
    """PersonaLabæ ¸å¿ƒæ¥å£ï¼Œæä¾›ç®€æ´çš„memoryå’Œå¯¹è¯åŠŸèƒ½
    
    é»˜è®¤ä½¿ç”¨OpenAIï¼Œä».envæ–‡ä»¶è¯»å–API keyã€‚
    
    Example:
        # åŸºç¡€ç”¨æ³•ï¼ˆé»˜è®¤OpenAIï¼‰
        persona = Persona(agent_id="alice")  # ä».envè¯»å–OPENAI_API_KEY
        
        # æˆ–è€…æ˜ç¡®æŒ‡å®šLLMç±»å‹
        persona = Persona.create_openai(agent_id="alice")
        persona = Persona.create_anthropic(agent_id="bob")
        
        # ä½¿ç”¨
        response = persona.chat("æˆ‘å–œæ¬¢çˆ¬å±±")
    """
    
    def __init__(
        self, 
        agent_id: str,
        llm_client=None,
        llm_type: str = None,
        llm_function: Callable = None,
        data_dir: str = "data",
        show_retrieval: bool = False,
        **llm_kwargs
    ):
        """åˆå§‹åŒ–Persona
        
        Args:
            agent_id: æ™ºèƒ½ä½“ID
            llm_client: é¢„é…ç½®çš„LLMå®¢æˆ·ç«¯
            llm_type: LLMç±»å‹ ('openai', 'anthropic', 'custom')ï¼Œé»˜è®¤ä¸º'openai'
            llm_function: è‡ªå®šä¹‰LLMå‡½æ•° (llm_type='custom'æ—¶ä½¿ç”¨)
            data_dir: æ•°æ®ç›®å½•
            show_retrieval: æ˜¯å¦æ˜¾ç¤ºæ£€ç´¢è¿‡ç¨‹
            **llm_kwargs: LLMå®¢æˆ·ç«¯çš„é¢å¤–å‚æ•°
        """
        self.agent_id = agent_id
        self.show_retrieval = show_retrieval
        
        # åˆå§‹åŒ–Memoryå’ŒMemo
        self.memory = Memory(agent_id=agent_id)
        self.memo = Memo(agent_id=agent_id, data_dir=data_dir)
        
        # é…ç½®LLMå®¢æˆ·ç«¯
        if llm_client:
            self.llm_client = llm_client
        elif llm_type:
            if llm_type == "openai":
                self.llm_client = self._create_openai_client(**llm_kwargs)
            elif llm_type == "anthropic":
                self.llm_client = self._create_anthropic_client(**llm_kwargs)
            elif llm_type == "custom":
                if not llm_function:
                    raise ValueError("llm_function is required when llm_type='custom'")
                self.llm_client = CustomLLMClient(llm_function=llm_function, **llm_kwargs)
            else:
                raise ValueError(f"Unsupported llm_type: {llm_type}")
        else:
            # é»˜è®¤ä½¿ç”¨OpenAI
            self.llm_client = self._create_openai_client(**llm_kwargs)
    
    def _create_openai_client(self, **kwargs):
        """åˆ›å»ºOpenAIå®¢æˆ·ç«¯ï¼Œä»é…ç½®ä¸­è¯»å–API key"""
        openai_config = config.get_llm_config("openai")
        if not openai_config.get("api_key"):
            raise ValueError("OpenAI API key not found. Please set OPENAI_API_KEY in .env file")
        
        # åˆå¹¶é…ç½®å’Œç”¨æˆ·å‚æ•°
        client_config = {**openai_config, **kwargs}
        return OpenAIClient(**client_config)
    
    def _create_anthropic_client(self, **kwargs):
        """åˆ›å»ºAnthropicå®¢æˆ·ç«¯ï¼Œä»é…ç½®ä¸­è¯»å–API key"""
        anthropic_config = config.get_llm_config("anthropic")
        if not anthropic_config.get("api_key"):
            raise ValueError("Anthropic API key not found. Please set ANTHROPIC_API_KEY in .env file")
        
        # åˆå¹¶é…ç½®å’Œç”¨æˆ·å‚æ•°
        client_config = {**anthropic_config, **kwargs}
        return AnthropicClient(**client_config)
    

    
    @classmethod
    def create_openai(cls, agent_id: str, api_key: str = None, **kwargs) -> 'Persona':
        """åˆ›å»ºä½¿ç”¨OpenAIçš„Personaå®ä¾‹"""
        if api_key:
            client = OpenAIClient(api_key=api_key)
            return cls(agent_id=agent_id, llm_client=client, **kwargs)
        else:
            return cls(agent_id=agent_id, llm_type="openai", **kwargs)
    
    @classmethod  
    def create_anthropic(cls, agent_id: str, api_key: str = None, **kwargs) -> 'Persona':
        """åˆ›å»ºä½¿ç”¨Anthropicçš„Personaå®ä¾‹"""
        if api_key:
            client = AnthropicClient(api_key=api_key)
            return cls(agent_id=agent_id, llm_client=client, **kwargs)
        else:
            return cls(agent_id=agent_id, llm_type="anthropic", **kwargs)
    

        
    @classmethod
    def create_custom(cls, agent_id: str, llm_function: Callable, **kwargs) -> 'Persona':
        """åˆ›å»ºä½¿ç”¨è‡ªå®šä¹‰LLMå‡½æ•°çš„Personaå®ä¾‹"""
        client = CustomLLMClient(llm_function=llm_function)
        return cls(agent_id=agent_id, llm_client=client, **kwargs)
        
    @classmethod
    def create_mock(cls, agent_id: str, **kwargs) -> 'Persona':
        """åˆ›å»ºä½¿ç”¨Mock LLMçš„Personaå®ä¾‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        from ..llm.custom_client import create_mock_response
        client = CustomLLMClient(llm_function=create_mock_response)
        return cls(agent_id=agent_id, llm_client=client, **kwargs)

    def chat(self, message: str, learn: bool = True) -> str:
        """ä¸AIå¯¹è¯ï¼Œè‡ªåŠ¨æ£€ç´¢ç›¸å…³è®°å¿†å¹¶å­¦ä¹ """
        # 1. æ£€ç´¢ç›¸å…³å¯¹è¯
        retrieved_conversations = []
        if self.memo.conversations:
            search_results = self.memo.search_similar_conversations(message, top_k=3)
            retrieved_conversations = search_results
            
            if self.show_retrieval and retrieved_conversations:
                print(f"\nğŸ” æ£€ç´¢åˆ° {len(retrieved_conversations)} ä¸ªç›¸å…³å¯¹è¯:")
                for i, conv in enumerate(retrieved_conversations, 1):
                    print(f"  {i}. {conv['summary'][:50]}...")
                print()
        
        # 2. æ„å»ºå¸¦æ£€ç´¢å†…å®¹çš„æ¶ˆæ¯
        enhanced_message = message
        if retrieved_conversations:
            context = "\n".join([
                f"ç›¸å…³å†å²: {conv['summary']}" 
                for conv in retrieved_conversations
            ])
            enhanced_message = f"{message}\n\nç›¸å…³èƒŒæ™¯:\n{context}"
        
        # 3. è·å–memory context
        memory_context = self._get_memory_context()
        
        # 4. æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ‹¥æœ‰å…³äºç”¨æˆ·çš„é•¿æœŸè®°å¿†ã€‚

ç”¨æˆ·è®°å¿†ä¿¡æ¯:
{memory_context}

è¯·åŸºäºä½ å¯¹ç”¨æˆ·çš„äº†è§£ï¼Œæä¾›ä¸ªæ€§åŒ–çš„å›åº”ã€‚"""
        
        # 5. è°ƒç”¨LLM
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": enhanced_message}
        ]
        response = self.llm_client.chat_completion(messages)
        
        # 6. å­¦ä¹ å¯¹è¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if learn:
            self._learn_from_conversation(message, response.content)
        
        return response.content

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """æœç´¢ç›¸å…³è®°å¿†"""
        return self.memo.search_similar_conversations(query, top_k=top_k)
    
    def add_memory(self, content: str, memory_type: str = "fact") -> None:
        """æ·»åŠ è®°å¿†"""
        if memory_type == "fact":
            self.memory.add_facts([content])
        elif memory_type == "preference":
            self.memory.add_preferences([content])
        elif memory_type == "event":
            self.memory.add_events([content])
        elif memory_type == "tom":
            self.memory.add_tom([content])
        else:
            raise ValueError(f"Unsupported memory_type: {memory_type}")
    
    def get_memory(self) -> Dict:
        """è·å–æ‰€æœ‰è®°å¿†"""
        return {
            "facts": self.memory.get_facts(),
            "preferences": self.memory.get_preferences(), 
            "events": self.memory.get_events(),
            "tom": self.memory.get_tom()
        }
    
    def close(self) -> None:
        """å…³é—­æ‰€æœ‰èµ„æº"""
        self.memory.close()
        self.memo.close()
        if hasattr(self.llm_client, 'close'):
            self.llm_client.close()
    
    @contextmanager
    def session(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨ç®¡ç†èµ„æº"""
        try:
            yield self
        finally:
            self.close()
    
    # å†…éƒ¨æ–¹æ³•
    def _get_memory_context(self) -> str:
        """è·å–memory context"""
        context_parts = []
        
        facts = self.memory.get_facts()
        if facts:
            context_parts.append(f"å…³äºç”¨æˆ·çš„äº‹å®: {', '.join(facts)}")
        
        preferences = self.memory.get_preferences()
        if preferences:
            context_parts.append(f"ç”¨æˆ·åå¥½: {', '.join(preferences)}")
        
        events = self.memory.get_events()
        if events:
            context_parts.append(f"é‡è¦äº‹ä»¶: {', '.join(events)}")
        
        tom = self.memory.get_tom()
        if tom:
            context_parts.append(f"ç”¨æˆ·å¿ƒç†æ¨¡å‹: {', '.join(tom)}")
        
        return "\n".join(context_parts) if context_parts else "æš‚æ— ç”¨æˆ·è®°å¿†ä¿¡æ¯"
    
    def _learn_from_conversation(self, user_message: str, ai_response: str) -> None:
        """ä»å¯¹è¯ä¸­å­¦ä¹ """
        # è®°å½•å¯¹è¯
        self.memo.add_conversation(user_message, ai_response)
        
        # æå–å¹¶å­¦ä¹ memory
        learning_prompt = f"""åˆ†æä»¥ä¸‹å¯¹è¯ï¼Œæå–å¯ä»¥å­¦ä¹ çš„ä¿¡æ¯ï¼š

ç”¨æˆ·: {user_message}
åŠ©æ‰‹: {ai_response}

è¯·æå–ï¼š
1. å…³äºç”¨æˆ·çš„æ–°äº‹å®
2. ç”¨æˆ·çš„åå¥½
3. é‡è¦äº‹ä»¶
4. ç”¨æˆ·çš„æƒ³æ³•/æ„Ÿå—

è¿”å›JSONæ ¼å¼ï¼š
{{"facts": [], "preferences": [], "events": [], "tom": []}}"""
        
        try:
            learning_messages = [{"role": "user", "content": learning_prompt}]
            learning_response = self.llm_client.chat_completion(learning_messages)
            learning_data = json.loads(learning_response.content)
            
            # æ·»åŠ åˆ°memory
            if learning_data.get("facts"):
                self.memory.add_facts(learning_data["facts"])
            if learning_data.get("preferences"):
                self.memory.add_preferences(learning_data["preferences"])
            if learning_data.get("events"):
                self.memory.add_events(learning_data["events"])
            if learning_data.get("tom"):
                self.memory.add_tom(learning_data["tom"])
                
        except Exception as e:
            # å­¦ä¹ å¤±è´¥ä¸å½±å“å¯¹è¯
            if self.show_retrieval:
                print(f"âš ï¸ å­¦ä¹ å¤±è´¥: {e}") 
#!/usr/bin/env python3
"""
Pipeline Debug Example

Shows detailed output from each stage of the memory update pipeline.
"""

from personalab.memory import Memory, MemoryUpdatePipeline
from personalab.llm import create_llm_client
from personalab.config import config
import json


def debug_pipeline_stages():
    """Debug example showing each pipeline stage result"""
    
    print("=== Pipeline Stage-by-Stage Debug ===\n")
    
    # 1. Setup
    print("1. Initial Setup")
    memory = Memory("debug_user")
    memory.update_profile("ç”¨æˆ·æ˜¯ä¸€åPythonå¼€å‘è€…ï¼Œå¯¹AIæŠ€æœ¯æ„Ÿå…´è¶£")
    memory.update_events(["ç”¨æˆ·è¯¢é—®äº†å…³äºæœºå™¨å­¦ä¹ çš„é—®é¢˜", "ç”¨æˆ·æåˆ°æƒ³å­¦ä¹ æ·±åº¦å­¦ä¹ "])
    
    print(f"Initial Profile: {memory.get_profile_content()}")
    print(f"Initial Events: {memory.get_event_content()}")
    print()
    
    # 2. Create pipeline
    print("2. Pipeline Setup")
    try:
        if config.validate_llm_config("openai"):
            llm_client = create_llm_client("openai", **config.get_llm_config("openai"))
            pipeline = MemoryUpdatePipeline(llm_client)
            print("âœ“ LLM client configured")
        else:
            raise Exception("No API key configured")
    except Exception as e:
        print(f"âš ï¸ Using mock pipeline (no LLM): {e}")
        pipeline = MemoryUpdatePipeline()
    print()
    
    # 3. Test conversation
    conversation = [
        {"role": "user", "content": "æˆ‘æœ€è¿‘åœ¨ç ”ç©¶transformeræ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯BERTå’ŒGPTçš„åŒºåˆ«"},
        {"role": "assistant", "content": "å¾ˆå¥½çš„ç ”ç©¶æ–¹å‘ï¼BERTæ˜¯åŒå‘ç¼–ç å™¨ï¼ŒGPTæ˜¯è‡ªå›å½’ç”Ÿæˆæ¨¡å‹"},
        {"role": "user", "content": "æˆ‘åœ¨ç”¨PyTorchå®ç°ä¸€ä¸ªç®€å•çš„transformerï¼Œé‡åˆ°äº†ä¸€äº›attentionæœºåˆ¶çš„é—®é¢˜"},
        {"role": "assistant", "content": "attentionæœºåˆ¶ç¡®å®æ˜¯å…³é”®éƒ¨åˆ†ï¼Œä½ å…·ä½“é‡åˆ°äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ"},
        {"role": "user", "content": "ä¸»è¦æ˜¯å¤šå¤´æ³¨æ„åŠ›çš„è®¡ç®—ï¼Œæˆ‘ä¸ç¡®å®šhead dimensionçš„è®¾ç½®"}
    ]
    
    print("3. Test Conversation:")
    for i, msg in enumerate(conversation, 1):
        role_color = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
        print(f"   {role_color} {msg['role']}: {msg['content']}")
    print()
    
    # Execute stages individually with detailed output
    try:
        print("="*60)
        print("STAGE 1: LLM MODIFICATION ANALYSIS")
        print("="*60)
        
        modification_result = pipeline.llm_modification_stage(memory, conversation)
        
        print("ğŸ“‹ Stage 1 Result (str):")
        print(f"Type: {type(modification_result)}")
        print(f"Content:\n{modification_result}")
        print(f"Length: {len(modification_result) if modification_result else 0} characters")
        print()
        
        print("="*60)
        print("STAGE 2: LLM MEMORY UPDATE")
        print("="*60)
        
        update_result = pipeline.llm_update_stage(memory, modification_result)
        
        print("ğŸ“‹ Stage 2 Result (UpdateResult):")
        print(f"Type: {type(update_result)}")
        print(f"Profile Updated: {update_result.profile_updated}")
        print(f"New Profile Content:\n{update_result.profile.get_content()}")
        print(f"New Events: {update_result.events.get_content()}")
        print(f"Raw LLM Response Length: {len(update_result.raw_llm_response)}")
        print(f"Metadata: {update_result.metadata}")
        print()
        
        print("="*60)
        print("STAGE 3: LLM THEORY OF MIND ANALYSIS")
        print("="*60)
        
        tom_result = pipeline.llm_theory_of_mind_stage(update_result, conversation)
        
        print("ğŸ“‹ Stage 3 Result (ToMResult):")
        print(f"Type: {type(tom_result)}")
        print(f"Insights Type: {type(tom_result.insights)}")
        print(f"Insights Content:\n{tom_result.insights}")
        print(f"Confidence Score: {tom_result.confidence_score}")
        print(f"Raw LLM Response Length: {len(tom_result.raw_llm_response)}")
        print(f"Metadata: {tom_result.metadata}")
        print()
        
        print("="*60)
        print("FINAL MEMORY STATE")
        print("="*60)
        
        # Create final memory
        new_memory = pipeline._create_updated_memory(memory, update_result, tom_result)
        
        print("ğŸ“‹ Final Memory:")
        print(f"Profile: {new_memory.get_profile_content()}")
        print(f"Events: {new_memory.get_event_content()}")
        print(f"ToM Metadata: {new_memory.tom_metadata}")
        print()
        
        print("ğŸ“Š COMPARISON:")
        print("Before â†’ After")
        print(f"Profile length: {len(memory.get_profile_content())} â†’ {len(new_memory.get_profile_content())}")
        print(f"Event count: {len(memory.get_event_content())} â†’ {len(new_memory.get_event_content())}")
        print(f"Has ToM data: {memory.tom_metadata is not None} â†’ {new_memory.tom_metadata is not None}")
        
    except Exception as e:
        print(f"âŒ Pipeline execution failed: {e}")
        print("This is expected if no LLM client is configured")
        print()
        print("To run with actual LLM processing:")
        print("1. Set up your .env file with OPENAI_API_KEY")
        print("2. Run: python setup_env.py")


def debug_complete_pipeline():
    """Debug the complete pipeline execution"""
    
    print("\n" + "="*60)
    print("COMPLETE PIPELINE EXECUTION")
    print("="*60)
    
    memory = Memory("complete_test")
    memory.update_profile("æµ‹è¯•ç”¨æˆ·")
    
    conversation = [
        {"role": "user", "content": "æˆ‘æƒ³å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†"},
        {"role": "assistant", "content": "NLPæ˜¯å¾ˆæœ‰è¶£çš„é¢†åŸŸï¼"}
    ]
    
    try:
        if config.validate_llm_config("openai"):
            llm_client = create_llm_client("openai", **config.get_llm_config("openai"))
            pipeline = MemoryUpdatePipeline(llm_client)
            
            print("ğŸš€ Executing complete pipeline...")
            new_memory, pipeline_result = pipeline.update_with_pipeline(memory, conversation)
            
            print("\nğŸ“‹ COMPLETE PIPELINE RESULT:")
            print(f"Type: {type(pipeline_result)}")
            print()
            
            print("ğŸ”„ Modification Result:")
            print(f"  Content: {pipeline_result.modification_result}")
            print()
            
            print("ğŸ“ Update Result:")
            print(f"  Profile Updated: {pipeline_result.update_result.profile_updated}")
            print(f"  New Profile: {new_memory.get_profile_content()}")
            print()
            
            print("ğŸ§  ToM Result:")
            print(f"  Insights: {pipeline_result.tom_result.insights}")
            print(f"  Confidence: {pipeline_result.tom_result.confidence_score}")
            print()
            
            print("ğŸ”§ Pipeline Metadata:")
            for key, value in pipeline_result.pipeline_metadata.items():
                print(f"  {key}: {value}")
                
        else:
            print("âš ï¸ No LLM configuration found")
            
    except Exception as e:
        print(f"âŒ Complete pipeline failed: {e}")


if __name__ == "__main__":
    debug_pipeline_stages()
    debug_complete_pipeline() 